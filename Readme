2015.10.22:
Use per-store avg sales as baseline. 
Avg sales are calculated based on data from open store.
Closed stores are ignored.
Score 0.25302

2015.10.25:
Use per-store avg sales to present store
Other features are mostly  OHE'd
Use linear regression to make prediction
No improvement observed for Ridge/ElasticNet

Sore: 0.19980

2015.10.27:
Use per-store median sales to represent stores,
instead of avg sales as 10.25.
Result is worse than using avg. 
Consider using per-store per-day median sales.

2015.10.28:
Added competition distance into feature list.
Using avg sales, score improved.
Score: 0.19977

When using random forest as learn algo, further
improved socre significantly.
Score: 0.14988

2015.10.30:
Using log sales improves result
Score: 0.14329

CV observations:
Competition distance:
use log: 0.173077
use abs: 0.173084
use log+abs: 0.173069 <---- So use both

Promo2 indication alone:
use: 0.173069   <---- So use Promo2 indication
not use: 0.173075

2015.11.22
xgboost:
Using sample script from kaggle: 0.11569
Using previous feature engineering: 0.14336
=> more features should help

2015.11.22
xgb codes:
preprocessing: main_xgb_preprocess.py
learn: main_learn_xgb.py

Added all store info features
Also added avg sales info into features
Improved score to 0.11569


2015.11.24:
Features: 
added state holiday, week of year, and corrected
days of week (based on true date)

xgb score improved with these addtional features
kaggle score: 0.11322

Algo:
aded ensemble (main_learn_ensemble.py)
initial test of averaging 6 xgb results
improved score to: 0.11038 

2015.12.02:
incremental addition of features:
using baseline & date based split:  0.11680 (20151201_1)
adding state holidays:



TOOD: 
- Use predicted sales to replace avg sales
